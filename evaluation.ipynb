{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f19079",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18504f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHROMA_PATH: c:\\My Projects\\RepoMind\\data\\chromadb\n",
      "‚öôÔ∏è Evaluation using TOP_K=15, RERANK_TOP_K=5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# So \"src\" imports work when running from project root\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "\n",
    "from src.database import initialize_database\n",
    "from src.retrieval import Retriever\n",
    "from src.llm import LLMEngine\n",
    "from src.config import CHROMA_PATH  # just to confirm path / debug\n",
    "from src.config import TOP_K, RERANK_TOP_K\n",
    "\n",
    "\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"CHROMA_PATH:\", CHROMA_PATH)\n",
    "print(f\"‚öôÔ∏è Evaluation using TOP_K={TOP_K}, RERANK_TOP_K={RERANK_TOP_K}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1462bb7",
   "metadata": {},
   "source": [
    "## Initialize Embeddings, Index, and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb7cd911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing embedding model and vector store...\n",
      "üîÑ Loading Embedding Model: BAAI/bge-small-en-v1.5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaca57f621994de4a4cdd6def0de2e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e53b575f544484962f5c8200b2c6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b47855dcee14a13941afc5fff0610e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054b9090e68b4434a523cdaaad379398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62ec73a26db45418fb01cb8d1c2a638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f5e42f9a0645ddbe2cfa58db43ba89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edb85b837604c2486b45b361233dd57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c417f214d74032840ec229a5be0af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4497bd02c143298200c42ed40cbe8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83d7e445eb04a92add8ad9159ebb829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e357569b3f4e37969c764a134e2fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding Model Loaded.\n",
      "üîç Initializing Retriever (with reranker)...\n",
      "üìÇ Loading Index from c:\\My Projects\\RepoMind\\data\\chromadb...\n",
      "‚ö†Ô∏è No index metadata found, creating index from vector store...\n",
      "‚úÖ Index created from existing vector store\n",
      "üöÄ Initializing Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e348d853ff48459c1ce970f539f522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520c8cb3c6e344c48c12ed91c76cf6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e68affa9ef04eb5b7f52d96df6cb6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c0022bc7a649ce8fb31ceacffc0631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26863de06db44b53bd67ee4876657891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e2dca4481c4827b6fabc8efbefa61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e839cacdfa5a48528f1466a98cd52e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Initializing LLMEngine (for RAG answers)...\n",
      "üß† Initializing LLM: llama-3.3-70b-versatile...\n",
      "‚úÖ RAG stack ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Initializing embedding model and vector store...\")\n",
    "initialize_database()  # sets global Settings.embed_model and returns vector store\n",
    "\n",
    "print(\"üîç Initializing Retriever (with reranker)...\")\n",
    "retriever = Retriever(use_reranker=True)\n",
    "\n",
    "print(\"üß† Initializing LLMEngine (for RAG answers)...\")\n",
    "llm_engine = LLMEngine()\n",
    "\n",
    "print(\"‚úÖ RAG stack ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63529e9c",
   "metadata": {},
   "source": [
    "## Test Definitions Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa5f9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 14 tests from data/tests.jsonl\n"
     ]
    }
   ],
   "source": [
    "class TestQuestion(BaseModel):\n",
    "    question: str\n",
    "    reference_answer: str\n",
    "    keywords: List[str] = []\n",
    "    category: Optional[str] = None\n",
    "\n",
    "\n",
    "def load_tests(path: str = \"data/tests.jsonl\") -> List[TestQuestion]:\n",
    "    \"\"\"\n",
    "    Load test questions from a JSONL file.\n",
    "    Each line: {\"question\": \"...\", \"reference_answer\": \"...\", \"keywords\": [...], \"category\": \"...\"}\n",
    "    \"\"\"\n",
    "    tests: List[TestQuestion] = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            tests.append(TestQuestion.model_validate_json(line))\n",
    "    print(f\"‚úÖ Loaded {len(tests)} tests from {path}\")\n",
    "    return tests\n",
    "\n",
    "# Quick sanity check (will fail if file doesn't exist)\n",
    "try:\n",
    "    _ = load_tests()\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è data/tests.jsonl not found yet ‚Äì create it before running full evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac7c76",
   "metadata": {},
   "source": [
    "## RAG Glue: Fetch Context & Answer Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b17d7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_context(question: str, top_k: int | None = None, use_rerank: bool | None = None):\n",
    "    \"\"\"\n",
    "    Use your Retriever to fetch relevant nodes for a question.\n",
    "\n",
    "    Args:\n",
    "        question: user query\n",
    "        top_k: number of initial candidates (defaults to TOP_K from config if None)\n",
    "        use_rerank: whether to apply cross-encoder reranking\n",
    "    \"\"\"\n",
    "    nodes = retriever.search(\n",
    "        query_text=question,\n",
    "        top_k=top_k,          # this becomes initial_k or TOP_K inside Retriever\n",
    "        rerank=use_rerank,    # controls whether SentenceTransformerRerank is applied\n",
    "    )\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def answer_question(question: str):\n",
    "    \"\"\"\n",
    "    Use your full RAG pipeline to answer and also return retrieved nodes.\n",
    "    \"\"\"\n",
    "    nodes = fetch_context(question)\n",
    "    answer = llm_engine.chat(question, nodes)\n",
    "    return answer, nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c52f3c7",
   "metadata": {},
   "source": [
    "## Metrics Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "438937e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalEval(BaseModel):\n",
    "    \"\"\"Evaluation metrics for retrieval performance.\"\"\"\n",
    "\n",
    "    mrr: float = Field(description=\"Mean Reciprocal Rank - average across all keywords\")\n",
    "    ndcg: float = Field(description=\"Normalized Discounted Cumulative Gain (binary relevance)\")\n",
    "    keywords_found: int = Field(description=\"Number of keywords found in top-k results\")\n",
    "    total_keywords: int = Field(description=\"Total number of keywords to find\")\n",
    "    keyword_coverage: float = Field(description=\"Percentage of keywords found\")\n",
    "\n",
    "\n",
    "class AnswerEval(BaseModel):\n",
    "    \"\"\"LLM-as-a-judge evaluation of answer quality.\"\"\"\n",
    "\n",
    "    feedback: str = Field(\n",
    "        description=\"Concise feedback on the answer quality, comparing it to the reference answer and evaluating based on the retrieved context\"\n",
    "    )\n",
    "    accuracy: float = Field(\n",
    "        description=\"How factually correct is the answer compared to the reference answer? 1 (wrong. any wrong answer must score 1) to 5 (ideal - perfectly accurate).\"\n",
    "    )\n",
    "    completeness: float = Field(\n",
    "        description=\"How complete is the answer in addressing all aspects of the question? 1 (very poor) to 5 (ideal).\"\n",
    "    )\n",
    "    relevance: float = Field(\n",
    "        description=\"How relevant is the answer to the specific question asked? 1 (very poor) to 5 (ideal).\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd04b0c",
   "metadata": {},
   "source": [
    "## Node Text Helper + Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8290c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_text(node) -> str:\n",
    "    \"\"\"\n",
    "    Safely extract text content from a LlamaIndex node.\n",
    "    \"\"\"\n",
    "    if hasattr(node, \"get_content\"):\n",
    "        try:\n",
    "            return node.get_content()\n",
    "        except Exception:\n",
    "            pass\n",
    "    if hasattr(node, \"text\"):\n",
    "        return node.text\n",
    "    return str(node)\n",
    "\n",
    "\n",
    "def calculate_mrr(keyword: str, retrieved_nodes: List) -> float:\n",
    "    \"\"\"Calculate reciprocal rank for a single keyword (case-insensitive).\"\"\"\n",
    "    keyword_lower = keyword.lower()\n",
    "    for rank, node in enumerate(retrieved_nodes, start=1):\n",
    "        if keyword_lower in get_node_text(node).lower():\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def calculate_dcg(relevances: List[int], k: int) -> float:\n",
    "    \"\"\"Calculate Discounted Cumulative Gain.\"\"\"\n",
    "    dcg = 0.0\n",
    "    for i in range(min(k, len(relevances))):\n",
    "        dcg += relevances[i] / math.log2(i + 2)  # i+2 because rank starts at 1\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def calculate_ndcg(keyword: str, retrieved_nodes: List, k: int = 10) -> float:\n",
    "    \"\"\"Calculate nDCG for a single keyword (binary relevance, case-insensitive).\"\"\"\n",
    "    keyword_lower = keyword.lower()\n",
    "\n",
    "    # Binary relevance: 1 if keyword found, 0 otherwise\n",
    "    relevances = [\n",
    "        1 if keyword_lower in get_node_text(node).lower() else 0\n",
    "        for node in retrieved_nodes[:k]\n",
    "    ]\n",
    "\n",
    "    dcg = calculate_dcg(relevances, k)\n",
    "    ideal_relevances = sorted(relevances, reverse=True)\n",
    "    idcg = calculate_dcg(ideal_relevances, k)\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f6d019",
   "metadata": {},
   "source": [
    "## Single-Test Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7318ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(\n",
    "    test: TestQuestion,\n",
    "    use_rerank: bool = True,\n",
    ") -> RetrievalEval:\n",
    "    \"\"\"\n",
    "    Evaluate retrieval performance for a test question.\n",
    "\n",
    "    If use_rerank=True:\n",
    "        - We ask the retriever to rerank (cross-encoder)\n",
    "        - We evaluate on the top RERANK_TOP_K nodes\n",
    "\n",
    "    If use_rerank=False:\n",
    "        - We evaluate on the top TOP_K nodes from pure vector search\n",
    "    \"\"\"\n",
    "\n",
    "    if use_rerank:\n",
    "        # Retrieve with reranking\n",
    "        retrieved_nodes = fetch_context(\n",
    "            test.question,\n",
    "            top_k=TOP_K,        # number of initial vector candidates\n",
    "            use_rerank=True,    # apply cross-encoder\n",
    "        )\n",
    "        k_eval = min(RERANK_TOP_K, len(retrieved_nodes))\n",
    "    else:\n",
    "        # Retrieve without reranking (pure vector search)\n",
    "        retrieved_nodes = fetch_context(\n",
    "            test.question,\n",
    "            top_k=TOP_K,\n",
    "            use_rerank=False,\n",
    "        )\n",
    "        k_eval = min(TOP_K, len(retrieved_nodes))\n",
    "\n",
    "    # Slice to the eval window (k_eval)\n",
    "    eval_nodes = retrieved_nodes[:k_eval]\n",
    "\n",
    "    # MRR and nDCG across all keywords, evaluated over eval_nodes\n",
    "    mrr_scores = [calculate_mrr(keyword, eval_nodes) for keyword in test.keywords]\n",
    "    ndcg_scores = [calculate_ndcg(keyword, eval_nodes, k=k_eval) for keyword in test.keywords]\n",
    "\n",
    "    avg_mrr = sum(mrr_scores) / len(mrr_scores) if mrr_scores else 0.0\n",
    "    avg_ndcg = sum(ndcg_scores) / len(ndcg_scores) if ndcg_scores else 0.0\n",
    "\n",
    "    keywords_found = sum(1 for score in mrr_scores if score > 0)\n",
    "    total_keywords = len(test.keywords)\n",
    "    keyword_coverage = (keywords_found / total_keywords * 100) if total_keywords > 0 else 0.0\n",
    "\n",
    "    return RetrievalEval(\n",
    "        mrr=avg_mrr,\n",
    "        ndcg=avg_ndcg,\n",
    "        keywords_found=keywords_found,\n",
    "        total_keywords=total_keywords,\n",
    "        keyword_coverage=keyword_coverage,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d909daa",
   "metadata": {},
   "source": [
    "## Judge LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ecfcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßë‚Äç‚öñÔ∏è Initializing Judge LLM: llama-3.2-3b-instruct ...\n",
      "‚úÖ Judge LLM ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 ‚Äì Initialize Judge LLM (free, open-source model via Groq)\n",
    "\n",
    "JUDGE_MODEL_NAME = \"llama-3.2-3b-instruct\"  # free OSS model on Groq, adjust if needed\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"‚ùå GROQ_API_KEY not set in environment. Add it to your .env file.\")\n",
    "\n",
    "print(f\"üßë‚Äç‚öñÔ∏è Initializing Judge LLM: {JUDGE_MODEL_NAME} ...\")\n",
    "judge_llm = Groq(model=JUDGE_MODEL_NAME, api_key=groq_api_key)\n",
    "print(\"‚úÖ Judge LLM ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "446c1260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 ‚Äì Parse JSON from judge response into AnswerEval\n",
    "\n",
    "def parse_answer_eval_from_response(raw_content: str) -> AnswerEval:\n",
    "    \"\"\"\n",
    "    Extract JSON object from judge LLM output and parse into AnswerEval.\n",
    "    \"\"\"\n",
    "    # Try to find the first {...} block\n",
    "    match = re.search(r\"\\{.*\\}\", raw_content, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Could not find JSON object in judge response:\\n{raw_content}\")\n",
    "\n",
    "    json_str = match.group(0)\n",
    "    return AnswerEval.model_validate_json(json_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "703244fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 ‚Äì Evaluate answer quality for a single test (LLM-as-a-judge)\n",
    "\n",
    "def evaluate_answer(test: TestQuestion) -> tuple[AnswerEval, str, list]:\n",
    "    \"\"\"\n",
    "    Evaluate answer quality using judge LLM.\n",
    "\n",
    "    Returns:\n",
    "        (AnswerEval, generated_answer, retrieved_nodes)\n",
    "    \"\"\"\n",
    "    # 1. Use your RAG pipeline to answer\n",
    "    generated_answer, retrieved_nodes = answer_question(test.question)\n",
    "\n",
    "    # 2. Build judge prompt\n",
    "    system_prompt = (\n",
    "        \"You are an expert evaluator assessing the quality of answers.\\n\"\n",
    "        \"You MUST respond with a single JSON object with these keys:\\n\"\n",
    "        \"  feedback (string), accuracy (number), completeness (number), relevance (number).\\n\"\n",
    "        \"Each score is from 1 to 5. If the answer is wrong, accuracy MUST be 1.\\n\"\n",
    "        \"Do NOT include any text before or after the JSON.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Question:\n",
    "{test.question}\n",
    "\n",
    "Generated Answer:\n",
    "{generated_answer}\n",
    "\n",
    "Reference Answer:\n",
    "{test.reference_answer}\n",
    "\n",
    "Please evaluate the generated answer on three dimensions:\n",
    "1. Accuracy: How factually correct is it compared to the reference answer? Only give 5/5 for perfect answers.\n",
    "2. Completeness: How thoroughly does it address all aspects of the question, covering all the information from the reference answer?\n",
    "3. Relevance: How well does it directly answer the specific question asked, giving no additional information?\n",
    "\n",
    "Return ONLY a JSON object with keys: feedback, accuracy, completeness, relevance.\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=system_prompt),\n",
    "        ChatMessage(role=\"user\", content=user_prompt),\n",
    "    ]\n",
    "\n",
    "    # 3. Call judge LLM\n",
    "    response = judge_llm.chat(messages)\n",
    "    raw_content = response.message.content if hasattr(response, \"message\") else str(response)\n",
    "\n",
    "    # 4. Parse into AnswerEval\n",
    "    answer_eval = parse_answer_eval_from_response(raw_content)\n",
    "\n",
    "    return answer_eval, generated_answer, retrieved_nodes\n",
    "\n",
    "# Again, quick manual check if you have tests:\n",
    "# tests = load_tests()\n",
    "# eval_res, gen_ans, ctx = evaluate_answer(tests[0])\n",
    "# eval_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d9770a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 14 tests from data/tests.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7cf5e47ba84fe7b62cec2df59514c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieval Eval (rerank=True):   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for: 'What is the main purpose of the Requests library and what problem does it solve compared to using the Python standard library?'\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Collection expecting embedding with dimension of 1024, got 384",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Run both evaluations (once tests.jsonl is ready)\u001b[39;00m\n\u001b[32m     43\u001b[39m tests = load_tests()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m retrieval_reranked_df = \u001b[43mevaluate_all_retrieval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_rerank\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# retrieval_baseline_df = evaluate_all_retrieval(tests, use_rerank=False)\u001b[39;00m\n\u001b[32m     48\u001b[39m answer_df = evaluate_all_answers(tests)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mevaluate_all_retrieval\u001b[39m\u001b[34m(tests, use_rerank)\u001b[39m\n\u001b[32m      7\u001b[39m rows = []\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(tests, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRetrieval Eval (rerank=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muse_rerank\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     r = \u001b[43mevaluate_retrieval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_rerank\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_rerank\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     rows.append({\n\u001b[32m     11\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: i,\n\u001b[32m     12\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: test.question,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mkeyword_coverage\u001b[39m\u001b[33m\"\u001b[39m: r.keyword_coverage,\n\u001b[32m     20\u001b[39m     })\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(rows)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mevaluate_retrieval\u001b[39m\u001b[34m(test, use_rerank)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03mEvaluate retrieval performance for a test question.\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33;03m    - We evaluate on the top TOP_K nodes from pure vector search\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_rerank:\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Retrieve with reranking\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     retrieved_nodes = \u001b[43mfetch_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# number of initial vector candidates\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_rerank\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# apply cross-encoder\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     k_eval = \u001b[38;5;28mmin\u001b[39m(RERANK_TOP_K, \u001b[38;5;28mlen\u001b[39m(retrieved_nodes))\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Retrieve without reranking (pure vector search)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mfetch_context\u001b[39m\u001b[34m(question, top_k, use_rerank)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_context\u001b[39m(question: \u001b[38;5;28mstr\u001b[39m, top_k: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, use_rerank: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Use your Retriever to fetch relevant nodes for a question.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03m        use_rerank: whether to apply cross-encoder reranking\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     nodes = \u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# this becomes initial_k or TOP_K inside Retriever\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrerank\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_rerank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# controls whether SentenceTransformerRerank is applied\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\My Projects\\RepoMind\\src\\retrieval.py:102\u001b[39m, in \u001b[36mRetriever.search\u001b[39m\u001b[34m(self, query_text, top_k, rerank)\u001b[39m\n\u001b[32m     96\u001b[39m retriever = VectorIndexRetriever(\n\u001b[32m     97\u001b[39m     index=\u001b[38;5;28mself\u001b[39m._index,\n\u001b[32m     98\u001b[39m     similarity_top_k=initial_k,\n\u001b[32m     99\u001b[39m )\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# 2. Retrieve nodes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m nodes = \u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   üìä Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(nodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m vector matches...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# 3. Rerank (Refinement)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\My Projects\\RepoMind\\.venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\My Projects\\RepoMind\\.venv\\Lib\\site-packages\\llama_index\\core\\base\\base_retriever.py:210\u001b[39m, in \u001b[36mBaseRetriever.retrieve\u001b[39m\u001b[34m(self, str_or_query_bundle)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.as_trace(\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    207\u001b[39m         CBEventType.RETRIEVE,\n\u001b[32m    208\u001b[39m         payload={EventPayload.QUERY_STR: query_bundle.query_str},\n\u001b[32m    209\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m retrieve_event:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m         nodes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m         nodes = \u001b[38;5;28mself\u001b[39m._handle_recursive_retrieval(query_bundle, nodes)\n\u001b[32m    212\u001b[39m         retrieve_event.on_end(\n\u001b[32m    213\u001b[39m             payload={EventPayload.NODES: nodes},\n\u001b[32m    214\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\My Projects\\RepoMind\\.venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\My Projects\\RepoMind\\.venv\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py:104\u001b[39m, in \u001b[36mVectorIndexRetriever._retrieve\u001b[39m\u001b[34m(self, query_bundle)\u001b[39m\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_bundle.embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query_bundle.embedding_strs) > \u001b[32m0\u001b[39m:\n\u001b[32m     99\u001b[39m         query_bundle.embedding = (\n\u001b[32m    100\u001b[39m             \u001b[38;5;28mself\u001b[39m._embed_model.get_agg_embedding_from_queries(\n\u001b[32m    101\u001b[39m                 query_bundle.embedding_strs\n\u001b[32m    102\u001b[39m             )\n\u001b[32m    103\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_nodes_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\My Projects\\RepoMind\\.venv\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py:220\u001b[39m, in \u001b[36mVectorIndexRetriever._get_nodes_with_embeddings\u001b[39m\u001b[34m(self, query_bundle_with_embeddings)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_nodes_with_embeddings\u001b[39m(\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m, query_bundle_with_embeddings: QueryBundle\n\u001b[32m    218\u001b[39m ) -> List[NodeWithScore]:\n\u001b[32m    219\u001b[39m     query = \u001b[38;5;28mself\u001b[39m._build_vector_store_query(query_bundle_with_embeddings)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     query_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_vector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m     nodes_to_fetch = \u001b[38;5;28mself\u001b[39m._determine_nodes_to_fetch(query_result)\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nodes_to_fetch:\n\u001b[32m    224\u001b[39m         \u001b[38;5;66;03m# Fetch any missing nodes from the docstore and insert them into the query result\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\My Projects\\RepoMind\\.venv\\Lib\\site-packages\\llama_index\\vector_stores\\chroma\\base.py:396\u001b[39m, in \u001b[36mChromaVectorStore.query\u001b[39m\u001b[34m(self, query, **kwargs)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m query.mode == VectorStoreQueryMode.MMR:\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mmr_search(query, where, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\My Projects\\RepoMind\\.venv\\Lib\\site-packages\\llama_index\\vector_stores\\chroma\\base.py:414\u001b[39m, in \u001b[36mChromaVectorStore._query\u001b[39m\u001b[34m(self, query_embeddings, n_results, where, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._collection.query(\n\u001b[32m    408\u001b[39m         query_embeddings=query_embeddings,\n\u001b[32m    409\u001b[39m         n_results=n_results,\n\u001b[32m    410\u001b[39m         where=where,\n\u001b[32m    411\u001b[39m         **kwargs,\n\u001b[32m    412\u001b[39m     )\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    420\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m> Top \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results[\u001b[33m'\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m nodes:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    421\u001b[39m nodes = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\My Projects\\RepoMind\\.venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:227\u001b[39m, in \u001b[36mCollection.query\u001b[39m\u001b[34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[32m    192\u001b[39m \n\u001b[32m    193\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    212\u001b[39m \n\u001b[32m    213\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    215\u001b[39m query_request = \u001b[38;5;28mself\u001b[39m._validate_and_prepare_query_request(\n\u001b[32m    216\u001b[39m     query_embeddings=query_embeddings,\n\u001b[32m    217\u001b[39m     query_texts=query_texts,\n\u001b[32m   (...)\u001b[39m\u001b[32m    224\u001b[39m     include=include,\n\u001b[32m    225\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m query_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhere\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhere_document\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_query_response(\n\u001b[32m    240\u001b[39m     response=query_results, include=query_request[\u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    241\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\My Projects\\RepoMind\\.venv\\Lib\\site-packages\\chromadb\\api\\rust.py:539\u001b[39m, in \u001b[36mRustBindingsAPI._query\u001b[39m\u001b[34m(self, collection_id, query_embeddings, ids, n_results, where, where_document, include, tenant, database)\u001b[39m\n\u001b[32m    523\u001b[39m filtered_ids_amount = \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28mself\u001b[39m.product_telemetry_client.capture(\n\u001b[32m    525\u001b[39m     CollectionQueryEvent(\n\u001b[32m    526\u001b[39m         collection_uuid=\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[32m   (...)\u001b[39m\u001b[32m    536\u001b[39m     )\n\u001b[32m    537\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m rust_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\n\u001b[32m    552\u001b[39m     ids=rust_response.ids,\n\u001b[32m    553\u001b[39m     embeddings=rust_response.embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m    559\u001b[39m     distances=rust_response.distances,\n\u001b[32m    560\u001b[39m )\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Collection expecting embedding with dimension of 1024, got 384"
     ]
    }
   ],
   "source": [
    "# Cell 11 ‚Äì Run evaluation over all tests and aggregate results\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def evaluate_all_retrieval(tests: List[TestQuestion], use_rerank: bool = True) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for i, test in enumerate(tqdm(tests, desc=f\"Retrieval Eval (rerank={use_rerank})\")):\n",
    "        r = evaluate_retrieval(test, use_rerank=use_rerank)\n",
    "        rows.append({\n",
    "            \"index\": i,\n",
    "            \"question\": test.question,\n",
    "            \"category\": test.category,\n",
    "            \"use_rerank\": use_rerank,\n",
    "            \"mrr\": r.mrr,\n",
    "            \"ndcg\": r.ndcg,\n",
    "            \"keywords_found\": r.keywords_found,\n",
    "            \"total_keywords\": r.total_keywords,\n",
    "            \"keyword_coverage\": r.keyword_coverage,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_all_answers(tests: List[TestQuestion]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for i, test in enumerate(tqdm(tests, desc=\"Answer Eval\")):\n",
    "        a_eval, generated_answer, _ = evaluate_answer(test)\n",
    "        rows.append({\n",
    "            \"index\": i,\n",
    "            \"question\": test.question,\n",
    "            \"category\": test.category,\n",
    "            \"accuracy\": a_eval.accuracy,\n",
    "            \"completeness\": a_eval.completeness,\n",
    "            \"relevance\": a_eval.relevance,\n",
    "            \"feedback\": a_eval.feedback,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"reference_answer\": test.reference_answer,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Run both evaluations (once tests.jsonl is ready)\n",
    "tests = load_tests()\n",
    "\n",
    "retrieval_reranked_df = evaluate_all_retrieval(tests, use_rerank=True)\n",
    "# retrieval_baseline_df = evaluate_all_retrieval(tests, use_rerank=False)\n",
    "\n",
    "answer_df = evaluate_all_answers(tests)\n",
    "\n",
    "retrieval_reranked_df.head(), answer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06061a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 ‚Äì Print summary metrics\n",
    "print(\"üìä Retrieval Metrics Summary\")\n",
    "print(retrieval_reranked_df[[\"mrr\", \"ndcg\", \"keyword_coverage\"]].describe())\n",
    "\n",
    "\n",
    "print(\"\\nüìä Answer Metrics Summary\")\n",
    "print(answer_df[[\"accuracy\", \"completeness\", \"relevance\"]].describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
