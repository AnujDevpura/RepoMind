{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f19079",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18504f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHROMA_PATH: c:\\My Projects\\RepoMind\\data\\chromadb\n",
      "âš™ï¸ Evaluation using TOP_K=15, RERANK_TOP_K=5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# So \"src\" imports work when running from project root\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "\n",
    "from src.database import initialize_database\n",
    "from src.retrieval import Retriever\n",
    "from src.llm import LLMEngine\n",
    "from src.config import CHROMA_PATH  # just to confirm path / debug\n",
    "from src.config import TOP_K, RERANK_TOP_K\n",
    "\n",
    "\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"CHROMA_PATH:\", CHROMA_PATH)\n",
    "print(f\"âš™ï¸ Evaluation using TOP_K={TOP_K}, RERANK_TOP_K={RERANK_TOP_K}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1462bb7",
   "metadata": {},
   "source": [
    "## Initialize Embeddings, Index, and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb7cd911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Initializing embedding model and vector store...\n",
      "ðŸ”„ Loading Embedding Model: BAAI/bge-small-en-v1.5...\n",
      "âœ… Embedding Model Loaded.\n",
      "ðŸ” Initializing Retriever (with reranker)...\n",
      "ðŸ“‚ Loading Index from c:\\My Projects\\RepoMind\\data\\chromadb...\n",
      "âš ï¸ No index metadata found, creating index from vector store...\n",
      "âœ… Index created from existing vector store\n",
      "ðŸš€ Initializing Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2...\n",
      "ðŸ§  Initializing LLMEngine (for RAG answers)...\n",
      "ðŸ§  Initializing LLM: llama-3.3-70b-versatile...\n",
      "âœ… RAG stack ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”§ Initializing embedding model and vector store...\")\n",
    "initialize_database()  # sets global Settings.embed_model and returns vector store\n",
    "\n",
    "print(\"ðŸ” Initializing Retriever (with reranker)...\")\n",
    "retriever = Retriever(use_reranker=True)\n",
    "\n",
    "print(\"ðŸ§  Initializing LLMEngine (for RAG answers)...\")\n",
    "llm_engine = LLMEngine()\n",
    "\n",
    "print(\"âœ… RAG stack ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63529e9c",
   "metadata": {},
   "source": [
    "## Test Definitions Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa5f9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 14 tests from data/tests.jsonl\n"
     ]
    }
   ],
   "source": [
    "class TestQuestion(BaseModel):\n",
    "    question: str\n",
    "    reference_answer: str\n",
    "    keywords: List[str] = []\n",
    "    category: Optional[str] = None\n",
    "\n",
    "\n",
    "def load_tests(path: str = \"data/tests.jsonl\") -> List[TestQuestion]:\n",
    "    \"\"\"\n",
    "    Load test questions from a JSONL file.\n",
    "    Each line: {\"question\": \"...\", \"reference_answer\": \"...\", \"keywords\": [...], \"category\": \"...\"}\n",
    "    \"\"\"\n",
    "    tests: List[TestQuestion] = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            tests.append(TestQuestion.model_validate_json(line))\n",
    "    print(f\"âœ… Loaded {len(tests)} tests from {path}\")\n",
    "    return tests\n",
    "\n",
    "# Quick sanity check (will fail if file doesn't exist)\n",
    "try:\n",
    "    _ = load_tests()\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ data/tests.jsonl not found yet â€“ create it before running full evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac7c76",
   "metadata": {},
   "source": [
    "## RAG Glue: Fetch Context & Answer Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17d7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_context(question: str, top_k: int | None = None, use_rerank: bool | None = None):\n",
    "    \"\"\"\n",
    "    Use your Retriever to fetch relevant nodes for a question.\n",
    "\n",
    "    Args:\n",
    "        question: user query\n",
    "        top_k: number of initial candidates (defaults to TOP_K from config if None)\n",
    "        use_rerank: whether to apply cross-encoder reranking\n",
    "    \"\"\"\n",
    "    nodes = retriever.search(\n",
    "        query_text=question,\n",
    "        top_k=top_k,          # this becomes initial_k or TOP_K inside Retriever\n",
    "        rerank=use_rerank,    # controls whether SentenceTransformerRerank is applied\n",
    "    )\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def answer_question(question: str):\n",
    "    \"\"\"\n",
    "    Use your full RAG pipeline to answer and also return retrieved nodes.\n",
    "    \"\"\"\n",
    "    nodes = fetch_context(question)\n",
    "    answer = llm_engine.chat(question, nodes)\n",
    "    return answer, nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c52f3c7",
   "metadata": {},
   "source": [
    "## Metrics Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438937e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalEval(BaseModel):\n",
    "    \"\"\"Evaluation metrics for retrieval performance.\"\"\"\n",
    "\n",
    "    mrr: float = Field(description=\"Mean Reciprocal Rank - average across all keywords\")\n",
    "    ndcg: float = Field(description=\"Normalized Discounted Cumulative Gain (binary relevance)\")\n",
    "    keywords_found: int = Field(description=\"Number of keywords found in top-k results\")\n",
    "    total_keywords: int = Field(description=\"Total number of keywords to find\")\n",
    "    keyword_coverage: float = Field(description=\"Percentage of keywords found\")\n",
    "\n",
    "\n",
    "class AnswerEval(BaseModel):\n",
    "    \"\"\"LLM-as-a-judge evaluation of answer quality.\"\"\"\n",
    "\n",
    "    feedback: str = Field(\n",
    "        description=\"Concise feedback on the answer quality, comparing it to the reference answer and evaluating based on the retrieved context\"\n",
    "    )\n",
    "    accuracy: float = Field(\n",
    "        description=\"How factually correct is the answer compared to the reference answer? 1 (wrong. any wrong answer must score 1) to 5 (ideal - perfectly accurate).\"\n",
    "    )\n",
    "    completeness: float = Field(\n",
    "        description=\"How complete is the answer in addressing all aspects of the question? 1 (very poor) to 5 (ideal).\"\n",
    "    )\n",
    "    relevance: float = Field(\n",
    "        description=\"How relevant is the answer to the specific question asked? 1 (very poor) to 5 (ideal).\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd04b0c",
   "metadata": {},
   "source": [
    "## Node Text Helper + Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8290c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_text(node) -> str:\n",
    "    \"\"\"\n",
    "    Safely extract text content from a LlamaIndex node.\n",
    "    \"\"\"\n",
    "    if hasattr(node, \"get_content\"):\n",
    "        try:\n",
    "            return node.get_content()\n",
    "        except Exception:\n",
    "            pass\n",
    "    if hasattr(node, \"text\"):\n",
    "        return node.text\n",
    "    return str(node)\n",
    "\n",
    "\n",
    "def calculate_mrr(keyword: str, retrieved_nodes: List) -> float:\n",
    "    \"\"\"Calculate reciprocal rank for a single keyword (case-insensitive).\"\"\"\n",
    "    keyword_lower = keyword.lower()\n",
    "    for rank, node in enumerate(retrieved_nodes, start=1):\n",
    "        if keyword_lower in get_node_text(node).lower():\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def calculate_dcg(relevances: List[int], k: int) -> float:\n",
    "    \"\"\"Calculate Discounted Cumulative Gain.\"\"\"\n",
    "    dcg = 0.0\n",
    "    for i in range(min(k, len(relevances))):\n",
    "        dcg += relevances[i] / math.log2(i + 2)  # i+2 because rank starts at 1\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def calculate_ndcg(keyword: str, retrieved_nodes: List, k: int = 10) -> float:\n",
    "    \"\"\"Calculate nDCG for a single keyword (binary relevance, case-insensitive).\"\"\"\n",
    "    keyword_lower = keyword.lower()\n",
    "\n",
    "    # Binary relevance: 1 if keyword found, 0 otherwise\n",
    "    relevances = [\n",
    "        1 if keyword_lower in get_node_text(node).lower() else 0\n",
    "        for node in retrieved_nodes[:k]\n",
    "    ]\n",
    "\n",
    "    dcg = calculate_dcg(relevances, k)\n",
    "    ideal_relevances = sorted(relevances, reverse=True)\n",
    "    idcg = calculate_dcg(ideal_relevances, k)\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f6d019",
   "metadata": {},
   "source": [
    "## Single-Test Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7318ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(\n",
    "    test: TestQuestion,\n",
    "    use_rerank: bool = True,\n",
    ") -> RetrievalEval:\n",
    "    \"\"\"\n",
    "    Evaluate retrieval performance for a test question.\n",
    "\n",
    "    If use_rerank=True:\n",
    "        - We ask the retriever to rerank (cross-encoder)\n",
    "        - We evaluate on the top RERANK_TOP_K nodes\n",
    "\n",
    "    If use_rerank=False:\n",
    "        - We evaluate on the top TOP_K nodes from pure vector search\n",
    "    \"\"\"\n",
    "\n",
    "    if use_rerank:\n",
    "        # Retrieve with reranking\n",
    "        retrieved_nodes = fetch_context(\n",
    "            test.question,\n",
    "            top_k=TOP_K,        # number of initial vector candidates\n",
    "            use_rerank=True,    # apply cross-encoder\n",
    "        )\n",
    "        k_eval = min(RERANK_TOP_K, len(retrieved_nodes))\n",
    "    else:\n",
    "        # Retrieve without reranking (pure vector search)\n",
    "        retrieved_nodes = fetch_context(\n",
    "            test.question,\n",
    "            top_k=TOP_K,\n",
    "            use_rerank=False,\n",
    "        )\n",
    "        k_eval = min(TOP_K, len(retrieved_nodes))\n",
    "\n",
    "    # Slice to the eval window (k_eval)\n",
    "    eval_nodes = retrieved_nodes[:k_eval]\n",
    "\n",
    "    # MRR and nDCG across all keywords, evaluated over eval_nodes\n",
    "    mrr_scores = [calculate_mrr(keyword, eval_nodes) for keyword in test.keywords]\n",
    "    ndcg_scores = [calculate_ndcg(keyword, eval_nodes, k=k_eval) for keyword in test.keywords]\n",
    "\n",
    "    avg_mrr = sum(mrr_scores) / len(mrr_scores) if mrr_scores else 0.0\n",
    "    avg_ndcg = sum(ndcg_scores) / len(ndcg_scores) if ndcg_scores else 0.0\n",
    "\n",
    "    keywords_found = sum(1 for score in mrr_scores if score > 0)\n",
    "    total_keywords = len(test.keywords)\n",
    "    keyword_coverage = (keywords_found / total_keywords * 100) if total_keywords > 0 else 0.0\n",
    "\n",
    "    return RetrievalEval(\n",
    "        mrr=avg_mrr,\n",
    "        ndcg=avg_ndcg,\n",
    "        keywords_found=keywords_found,\n",
    "        total_keywords=total_keywords,\n",
    "        keyword_coverage=keyword_coverage,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d909daa",
   "metadata": {},
   "source": [
    "## Judge LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1ecfcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§‘â€âš–ï¸ Initializing Judge LLM via Ollama: llama3.1 ...\n",
      "âœ… Judge LLM ready!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"  # default Ollama endpoint\n",
    "\n",
    "# Initialize client (OpenAI-compatible API) â€“ works if you're already configured\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
    "\n",
    "JUDGE_MODEL_NAME = \"llama3.1\"\n",
    "\n",
    "print(f\"ðŸ§‘â€âš–ï¸ Initializing Judge LLM via Ollama: {JUDGE_MODEL_NAME} ...\")\n",
    "\n",
    "def call_ollama_judge(messages):\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=JUDGE_MODEL_NAME,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"âœ… Judge LLM ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b72c266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ok\": true}\n"
     ]
    }
   ],
   "source": [
    "#sanity test\n",
    "\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Respond with a JSON object: {\\\"ok\\\": true} and nothing else.\"}\n",
    "]\n",
    "\n",
    "print(call_ollama_judge(test_messages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "446c1260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 â€“ Parse JSON from judge response into AnswerEval\n",
    "\n",
    "def parse_answer_eval_from_response(raw_content: str) -> AnswerEval:\n",
    "    \"\"\"\n",
    "    Extract JSON object from judge LLM output and parse into AnswerEval.\n",
    "    \"\"\"\n",
    "    # Try to find the first {...} block\n",
    "    match = re.search(r\"\\{.*\\}\", raw_content, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Could not find JSON object in judge response:\\n{raw_content}\")\n",
    "\n",
    "    json_str = match.group(0)\n",
    "    return AnswerEval.model_validate_json(json_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "703244fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 â€“ Evaluate answer quality for a single test (LLM-as-a-judge)\n",
    "\n",
    "def evaluate_answer(test: TestQuestion) -> tuple[AnswerEval, str, list]:\n",
    "    \"\"\"\n",
    "    Evaluate answer quality using judge LLM.\n",
    "\n",
    "    Returns:\n",
    "        (AnswerEval, generated_answer, retrieved_nodes)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Use your RAG pipeline to answer\n",
    "    generated_answer, retrieved_nodes = answer_question(test.question)\n",
    "\n",
    "    # 2. Build judge prompt\n",
    "\n",
    "    system_prompt = (\n",
    "            \"You are an expert evaluator assessing the quality of answers.\\n\"\n",
    "            \"You MUST respond with a single JSON object with these keys:\\n\"\n",
    "            \"  feedback (string), accuracy (number), completeness (number), relevance (number).\\n\"\n",
    "            \"Each score is from 1 to 5. If the answer is wrong, accuracy MUST be 1.\\n\"\n",
    "            \"Do NOT include any text before or after the JSON.\"\n",
    "        )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Question:\n",
    "    {test.question}\n",
    "\n",
    "    Generated Answer:\n",
    "    {generated_answer}\n",
    "\n",
    "    Reference Answer:\n",
    "    {test.reference_answer}\n",
    "\n",
    "    Please evaluate the generated answer on three dimensions:\n",
    "    1. Accuracy: How factually correct is it compared to the reference answer? Only give 5/5 for perfect answers.\n",
    "    2. Completeness: How thoroughly does it address all aspects of the question, covering all the information from the reference answer?\n",
    "    3. Relevance: How well does it directly answer the specific question asked, giving no additional information?\n",
    "\n",
    "    Return ONLY a JSON object with keys: feedback, accuracy, completeness, relevance.\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. Call judge LLM via Ollama\n",
    "    judge_messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    raw_content = call_ollama_judge(judge_messages)\n",
    "\n",
    "    # 4. Parse into AnswerEval\n",
    "    answer_eval = parse_answer_eval_from_response(raw_content)\n",
    "\n",
    "    return answer_eval, generated_answer, retrieved_nodes\n",
    "\n",
    "# Again, quick manual check if you have tests:\n",
    "# tests = load_tests()\n",
    "# eval_res, gen_ans, ctx = evaluate_answer(tests[0])\n",
    "# eval_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d9770a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 14 tests from data/tests.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380f4e6f90684c0192c37694d2d023ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieval Eval (rerank=True):   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for: 'What is the main purpose of the Requests library and what problem does it solve compared to using the Python standard library?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'How do you perform a simple GET request with Requests and inspect the basic attributes of the Response object?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'How does Requests support different HTTP methods beyond GET, and where in the codebase are these higher-level helpers defined?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'What is a Session in Requests, why would you use it, and where is the core Session implementation located in the repository?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'How does Requests handle cookies and cookie persistence between requests?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'How does Requests integrate with urllib3 to provide connection pooling and keep-alive behavior, and which module is responsible for this?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'Where in the Requests repository would you look to understand the core Request and Response data structures?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'How does Requests support authentication, such as basic or digest auth, and where is this logic implemented?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'What features does Requests provide for handling SSL/TLS verification, and how are these configured by default?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'How can you send query parameters in the URL with Requests, and how does the library handle encoding of these parameters?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'How can you send JSON data in a POST request using Requests, and why is using the json parameter preferred over manually encoding the body?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'How are errors and exceptional situations represented in Requests, and where would you look in the codebase to see the exception hierarchy?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'Where is the top-level project metadata like license and high-level description located in the Requests repository, and what license does it use?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'How is the Requests repository structured in terms of main package code versus tests and documentation directories?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa22ea4d1044ae0a9a71c1bc754b034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Answer Eval:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for: 'What is the main purpose of the Requests library and what problem does it solve compared to using the Python standard library?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'How do you perform a simple GET request with Requests and inspect the basic attributes of the Response object?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n",
      "ðŸ” Searching for: 'How does Requests support different HTTP methods beyond GET, and where in the codebase are these higher-level helpers defined?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99205, Requested 3380. Please try again in 37m13.44s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.6669944029647263 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99203, Requested 3380. Please try again in 37m11.712s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for: 'What is a Session in Requests, why would you use it, and where is the core Session implementation located in the repository?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99187, Requested 1051. Please try again in 3m25.632s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.2187406971134827 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99186, Requested 1051. Please try again in 3m24.768s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for: 'How does Requests handle cookies and cookie persistence between requests?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99171, Requested 2582. Please try again in 25m14.592s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.0943990586561805 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99170, Requested 2582. Please try again in 25m13.728s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for: 'How does Requests integrate with urllib3 to provide connection pooling and keep-alive behavior, and which module is responsible for this?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99155, Requested 2482. Please try again in 23m34.368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.7570720974911924 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99153, Requested 2482. Please try again in 23m32.64s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for: 'Where in the Requests repository would you look to understand the core Request and Response data structures?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99138, Requested 2854. Please try again in 28m41.088s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.238990016153091 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99136, Requested 2854. Please try again in 28m39.36s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for: 'How does Requests support authentication, such as basic or digest auth, and where is this logic implemented?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99125, Requested 2319. Please try again in 20m47.616s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.9072954172786747 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99124, Requested 2319. Please try again in 20m46.752s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for: 'What features does Requests provide for handling SSL/TLS verification, and how are these configured by default?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99103, Requested 2409. Please try again in 21m46.368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.32106836974913 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99102, Requested 2409. Please try again in 21m45.504s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for: 'How can you send query parameters in the URL with Requests, and how does the library handle encoding of these parameters?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99089, Requested 3191. Please try again in 32m49.92s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.5831252721445517 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99088, Requested 3191. Please try again in 32m49.056s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for: 'How can you send JSON data in a POST request using Requests, and why is using the json parameter preferred over manually encoding the body?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99075, Requested 3366. Please try again in 35m9.024s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.8649279224512658 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99073, Requested 3366. Please try again in 35m7.295999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for: 'How are errors and exceptional situations represented in Requests, and where would you look in the codebase to see the exception hierarchy?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99060, Requested 3237. Please try again in 33m4.608s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.816892004018064 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99058, Requested 3237. Please try again in 33m2.88s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for: 'Where is the top-level project metadata like license and high-level description located in the Requests repository, and what license does it use?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99044, Requested 1990. Please try again in 14m53.376s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.7279608577408712 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99043, Requested 1990. Please try again in 14m52.512s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for: 'How is the Requests repository structured in terms of main package code versus tests and documentation directories?'\n",
      "   ðŸ“Š Found 30 vector matches...\n",
      "   âœ¨ Reranking to top 5 results...\n",
      "   âœ… Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99029, Requested 2288. Please try again in 18m57.888s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.2290440019491253 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kb7ee3bke7ybb68pgv52sy2d` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99027, Requested 2288. Please try again in 18m56.16s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   index                                           question      category  \\\n",
       " 0      0  What is the main purpose of the Requests libra...      overview   \n",
       " 1      1  How do you perform a simple GET request with R...   basic_usage   \n",
       " 2      2  How does Requests support different HTTP metho...  architecture   \n",
       " 3      3  What is a Session in Requests, why would you u...      sessions   \n",
       " 4      4  How does Requests handle cookies and cookie pe...       cookies   \n",
       " \n",
       "    use_rerank       mrr      ndcg  keywords_found  total_keywords  \\\n",
       " 0        True  0.200000  0.252372               2               5   \n",
       " 1        True  0.338889  0.501279               5               6   \n",
       " 2        True  0.647619  0.674400               6               7   \n",
       " 3        True  0.200000  0.200000               1               5   \n",
       " 4        True  0.300000  0.346713               2               4   \n",
       " \n",
       "    keyword_coverage  \n",
       " 0         40.000000  \n",
       " 1         83.333333  \n",
       " 2         85.714286  \n",
       " 3         20.000000  \n",
       " 4         50.000000  ,\n",
       "    index                                           question      category  \\\n",
       " 0      0  What is the main purpose of the Requests libra...      overview   \n",
       " 1      1  How do you perform a simple GET request with R...   basic_usage   \n",
       " 2      2  How does Requests support different HTTP metho...  architecture   \n",
       " 3      3  What is a Session in Requests, why would you u...      sessions   \n",
       " 4      4  How does Requests handle cookies and cookie pe...       cookies   \n",
       " \n",
       "    accuracy  completeness  relevance  \\\n",
       " 0       4.0           5.0        3.0   \n",
       " 1       4.0           3.0        5.0   \n",
       " 2       1.0           0.0        1.0   \n",
       " 3       1.0           0.0        0.0   \n",
       " 4       1.0           2.0        1.0   \n",
       " \n",
       "                                             feedback  \\\n",
       " 0  The generated answer is mostly accurate and co...   \n",
       " 1  The generated answer is mostly correct but lac...   \n",
       " 2  The generated answer is not relevant to the qu...   \n",
       " 3  The generated answer was unable to provide an ...   \n",
       " 4  The generated answer is incomplete and inaccur...   \n",
       " \n",
       "                                     generated_answer  \\\n",
       " 0  The main purpose of the Requests library is to...   \n",
       " 1  In `src\\requests\\api.py`, you can perform a si...   \n",
       " 2  âŒ Error generating response: Error code: 429 -...   \n",
       " 3  âŒ Error generating response: Error code: 429 -...   \n",
       " 4  âŒ Error generating response: Error code: 429 -...   \n",
       " \n",
       "                                     reference_answer  \n",
       " 0  Requests is an HTTP client library for Python ...  \n",
       " 1  You import requests and call requests.get with...  \n",
       " 2  Requests provides top-level helper functions f...  \n",
       " 3  A Session in Requests represents a persistent ...  \n",
       " 4  Requests handles cookies through its cookie ja...  )"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 11 â€“ Run evaluation over all tests and aggregate results\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def evaluate_all_retrieval(tests: List[TestQuestion], use_rerank: bool = True) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for i, test in enumerate(tqdm(tests, desc=f\"Retrieval Eval (rerank={use_rerank})\")):\n",
    "        r = evaluate_retrieval(test, use_rerank=use_rerank)\n",
    "        rows.append({\n",
    "            \"index\": i,\n",
    "            \"question\": test.question,\n",
    "            \"category\": test.category,\n",
    "            \"use_rerank\": use_rerank,\n",
    "            \"mrr\": r.mrr,\n",
    "            \"ndcg\": r.ndcg,\n",
    "            \"keywords_found\": r.keywords_found,\n",
    "            \"total_keywords\": r.total_keywords,\n",
    "            \"keyword_coverage\": r.keyword_coverage,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_all_answers(tests: List[TestQuestion]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for i, test in enumerate(tqdm(tests, desc=\"Answer Eval\")):\n",
    "        a_eval, generated_answer, _ = evaluate_answer(test)\n",
    "        rows.append({\n",
    "            \"index\": i,\n",
    "            \"question\": test.question,\n",
    "            \"category\": test.category,\n",
    "            \"accuracy\": a_eval.accuracy,\n",
    "            \"completeness\": a_eval.completeness,\n",
    "            \"relevance\": a_eval.relevance,\n",
    "            \"feedback\": a_eval.feedback,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"reference_answer\": test.reference_answer,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Run both evaluations (once tests.jsonl is ready)\n",
    "tests = load_tests()\n",
    "\n",
    "retrieval_reranked_df = evaluate_all_retrieval(tests, use_rerank=True)\n",
    "# retrieval_baseline_df = evaluate_all_retrieval(tests, use_rerank=False)\n",
    "\n",
    "answer_df = evaluate_all_answers(tests)\n",
    "\n",
    "retrieval_reranked_df.head(), answer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06061a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Retrieval Metrics Summary\n",
      "             mrr       ndcg  keyword_coverage\n",
      "count  14.000000  14.000000         14.000000\n",
      "mean    0.408560   0.434960         53.860544\n",
      "std     0.248714   0.250121         29.875341\n",
      "min     0.000000   0.000000          0.000000\n",
      "25%     0.225000   0.275957         42.500000\n",
      "50%     0.437500   0.475618         55.000000\n",
      "75%     0.610417   0.645488         80.000000\n",
      "max     0.800000   0.800000         85.714286\n",
      "\n",
      "ðŸ“Š Answer Metrics Summary\n",
      "        accuracy  completeness  relevance\n",
      "count  14.000000     14.000000  14.000000\n",
      "mean    1.428571      1.785714   1.928571\n",
      "std     1.089410      1.311404   1.384768\n",
      "min     1.000000      0.000000   0.000000\n",
      "25%     1.000000      1.250000   1.000000\n",
      "50%     1.000000      2.000000   1.000000\n",
      "75%     1.000000      2.000000   3.000000\n",
      "max     4.000000      5.000000   5.000000\n"
     ]
    }
   ],
   "source": [
    "# Cell 12 â€“ Print summary metrics\n",
    "print(\"ðŸ“Š Retrieval Metrics Summary\")\n",
    "print(retrieval_reranked_df[[\"mrr\", \"ndcg\", \"keyword_coverage\"]].describe())\n",
    "\n",
    "\n",
    "print(\"\\nðŸ“Š Answer Metrics Summary\")\n",
    "print(answer_df[[\"accuracy\", \"completeness\", \"relevance\"]].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47542c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
