{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RepoMind RAG Evaluation Notebook\n",
    "\n",
    "This notebook evaluates the performance of the RAG pipeline on a target code repository. It performs the following steps:\n",
    "\n",
    "1.  **Setup & Ingestion**: Clones the target repository and ingests it into the vector database.\n",
    "2.  **Define Evaluation Dataset**: Creates a set of questions and ground truth answers to test the RAG system's understanding of the codebase.\n",
    "3.  **Run Pipeline**: For each question, it runs the full retrieval and generation pipeline.\n",
    "4.  **Evaluate with `ragas`**: Uses the `ragas` library to calculate key metrics like faithfulness, context precision, context recall, and answer relevancy.\n",
    "\n",
    "**Target Repository**: `https://github.com/psf/requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 22:36:12,394 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing database...\n",
      "üîÑ Loading Embedding Model: BAAI/bge-small-en-v1.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 22:36:18,775 - INFO - 1 prompt is loaded, with the key: query\n",
      "2025-12-01 22:36:19,415 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding Model Loaded.\n",
      "\n",
      "üöÄ Ingesting repository: https://github.com/psf/requests...\n",
      "\n",
      "============================================================\n",
      "üöÄ Starting ingestion for: https://github.com/psf/requests\n",
      "============================================================\n",
      "\n",
      "üìÇ Repo already exists at c:\\My Projects\\RepoMind\\data\\cloned_repos\\requests, skipping clone...\n",
      "üîç Scanning files in c:\\My Projects\\RepoMind\\data\\cloned_repos\\requests...\n",
      "‚úÖ Loaded 45 documents from 45 code files.\n",
      "‚ö†Ô∏è Skipped 2 files.\n",
      "\n",
      "üìÑ Processing 45 documents...\n",
      "‚úÇÔ∏è Chunking code files (AST-aware when possible)...\n",
      "  ‚ö†Ô∏è Skipping empty document: __init__.py\n",
      "  ‚ö†Ô∏è AST parsing failed for custom.css, using text fallback for this file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\My Projects\\RepoMind\\.venv\\Lib\\site-packages\\tree_sitter\\__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚ö†Ô∏è AST parsing failed for hacks.html, using text fallback for this file\n",
      "  ‚ö†Ô∏è AST parsing failed for sidebarintro.html, using text fallback for this file\n",
      "  ‚ö†Ô∏è AST parsing failed for sidebarlogo.html, using text fallback for this file\n",
      "  ‚úì AST-aware chunked 44 python files into 259 nodes\n",
      "üß© Created 259 semantic chunks.\n",
      "\n",
      "üíæ Saving to Vector Database (this may take a while)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02a786b8fd746a69c8afc4d0464f621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ Ingestion Complete! Embeddings stored in ChromaDB.\n",
      "   Repository: requests\n",
      "   Documents: 45\n",
      "   Chunks: 259\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.database import initialize_database\n",
    "from src.ingestion import ingest_repo\n",
    "from src.retrieval import Retriever\n",
    "from src.llm import LLMEngine\n",
    "\n",
    "# --- Configuration ---\n",
    "TEST_REPO_URL = \"https://github.com/psf/requests\"\n",
    "\n",
    "print(\"üîß Initializing database...\")\n",
    "initialize_database()\n",
    "\n",
    "print(f\"\\nüöÄ Ingesting repository: {TEST_REPO_URL}...\")\n",
    "try:\n",
    "    ingest_repo(TEST_REPO_URL, force_clone=False) # Set force_clone=True to re-download\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ingestion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize RAG Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 22:36:36,501 - INFO - Loading all indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading Index from c:\\My Projects\\RepoMind\\data\\chromadb...\n",
      "‚ö†Ô∏è No index metadata found, creating index from vector store...\n",
      "‚úÖ Index created from existing vector store\n",
      "üöÄ Initializing Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2...\n",
      "üß† Initializing LLM: openai/gpt-oss-120b...\n",
      "‚úÖ RAG components loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    retriever = Retriever(use_reranker=True)\n",
    "    llm_engine = LLMEngine()\n",
    "    print(\"‚úÖ RAG components loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize RAG components: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Evaluation Questions\n",
    "\n",
    "Here we define a list of questions to ask the RAG model. We also provide `ground_truth` answers, which are required by `ragas` to calculate `context_recall`. The ground truth should be a concise, factual statement that is expected to be found in the source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [\n",
    "    {\n",
    "        \"question\": \"What is the main purpose of the requests library?\",\n",
    "        \"ground_truth\": \"Requests is an HTTP library for Python, built for human beings. It allows you to send HTTP/1.1 requests extremely easily.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does the Session object persist parameters across requests?\",\n",
    "        \"ground_truth\": \"A Session object has a variety of methods for customizing requests, such as setting headers, auth, cookies, and proxies. These settings are persisted across all requests made with that session instance.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the role of the `requests.adapters.HTTPAdapter`?\",\n",
    "        \"ground_truth\": \"The HTTPAdapter is responsible for the actual transport of the request. It sends the request to the target server and handles connection pooling.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How can you specify a timeout for a request?\",\n",
    "        \"ground_truth\": \"You can tell Requests to stop waiting for a response after a given number of seconds with the `timeout` parameter. It can be a float for a connect and read timeout, or a tuple `(connect_timeout, read_timeout)`.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are cookies handled in the requests library?\",\n",
    "        \"ground_truth\": \"Cookies are returned in a `RequestsCookieJar`, which acts like a dictionary but also works across domains and paths. Session objects also persist cookies across all requests.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What file defines the main `requests.get` function?\",\n",
    "        \"ground_truth\": \"The `get` function is a wrapper defined in `requests/api.py` that calls the `request` function with the method set to 'GET'.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the RAG Pipeline and Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question: What is the main purpose of the requests library?\n",
      "üîç Searching for: 'What is the main purpose of the requests library?'\n",
      "   üìä Found 10 vector matches...\n",
      "   ‚ú® Reranking to top 5 results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b698268ad7b415fa4164c0f081f95ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 22:36:49,350 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Answer generated.\n",
      "\n",
      "Processing question: How does the Session object persist parameters across requests?\n",
      "üîç Searching for: 'How does the Session object persist parameters across requests?'\n",
      "   üìä Found 10 vector matches...\n",
      "   ‚ú® Reranking to top 5 results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03678bcc445745afbc84906d9be9380f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 22:36:51,647 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Answer generated.\n",
      "\n",
      "Processing question: What is the role of the `requests.adapters.HTTPAdapter`?\n",
      "üîç Searching for: 'What is the role of the `requests.adapters.HTTPAdapter`?'\n",
      "   üìä Found 10 vector matches...\n",
      "   ‚ú® Reranking to top 5 results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4944af3b275149a98de4144633e2e998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 22:36:52,151 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-12-01 22:36:52,153 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 22:37:00,386 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Answer generated.\n",
      "\n",
      "Processing question: How can you specify a timeout for a request?\n",
      "üîç Searching for: 'How can you specify a timeout for a request?'\n",
      "   üìä Found 10 vector matches...\n",
      "   ‚ú® Reranking to top 5 results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f4bb7bc9b74896bbfc507d3c214bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 22:37:00,926 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-12-01 22:37:00,926 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 22:37:23,041 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Answer generated.\n",
      "\n",
      "Processing question: How are cookies handled in the requests library?\n",
      "üîç Searching for: 'How are cookies handled in the requests library?'\n",
      "   üìä Found 10 vector matches...\n",
      "   ‚ú® Reranking to top 5 results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f687049eace94d6085fafae2717fc8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 22:37:23,603 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 22:37:23,604 - INFO - Retrying request to /chat/completions in 22.000000 seconds\n",
      "2025-12-01 22:37:48,987 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Answer generated.\n",
      "\n",
      "Processing question: What file defines the main `requests.get` function?\n",
      "üîç Searching for: 'What file defines the main `requests.get` function?'\n",
      "   üìä Found 10 vector matches...\n",
      "   ‚ú® Reranking to top 5 results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d6fc7f68bb495d878e9551a974d6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 22:37:49,522 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-12-01 22:37:49,522 - INFO - Retrying request to /chat/completions in 25.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Reranked to 5 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 22:38:15,827 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Answer generated.\n",
      "\n",
      "‚úÖ Pipeline execution complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'contexts', 'ground_truth'],\n",
       "    num_rows: 6\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for item in eval_questions:\n",
    "    question = item[\"question\"]\n",
    "    print(f\"\\nProcessing question: {question}\")\n",
    "    \n",
    "    # 1. Retrieve context\n",
    "    context_nodes = retriever.search(question)\n",
    "    contexts = [node.get_content() for node in context_nodes]\n",
    "    \n",
    "    # 2. Generate answer\n",
    "    answer = llm_engine.chat(question, context_nodes)\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truth\": item[\"ground_truth\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"  -> Answer generated.\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "results_df = pd.DataFrame(results)\n",
    "eval_dataset = Dataset.from_pandas(results_df)\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline execution complete.\")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate with `ragas`\n",
    "\n",
    "Now we use `ragas` to evaluate the collected responses. We will measure:\n",
    "\n",
    "- **Faithfulness**: How factually consistent is the answer with the provided context.\n",
    "- **Answer Relevancy**: How relevant is the answer to the question.\n",
    "- **Context Precision**: A measure of how relevant the retrieved contexts are.\n",
    "- **Context Recall**: Measures if all the necessary information from the `ground_truth` was retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131962ca06394307bad042a4f710dd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anujd\\AppData\\Local\\Temp\\ipykernel_2196\\28361334.py:49: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  judge_llm = LangchainLLMWrapper(judge_chat)\n",
      "2025-12-02 00:54:49,038 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-12-02 00:54:49,039 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Running FULL RAGAS evaluation on all samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec7b2df66344744b4d9a3e9b9fb7560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 00:54:59,153 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 00:55:20,849 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 00:55:40,640 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 00:57:09,982 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 00:59:18,927 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:00:21,113 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:02:00,424 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:03:47,225 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:06:01,924 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:06:06,226 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:06:10,383 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:07:46,223 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:08:38,750 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:09:07,747 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:09:27,579 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:09:50,930 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:10:26,790 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:11:24,882 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:13:56,545 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:16:08,155 - ERROR - Exception raised in Job[6]: TimeoutError()\n",
      "2025-12-02 01:16:31,556 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:16:31,843 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:16:36,257 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:16:57,479 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:17:16,840 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:17:36,838 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:18:00,098 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:18:21,438 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:19:34,860 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:21:54,301 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:23:48,771 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:25:31,964 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:25:35,912 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:25:39,613 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:25:59,940 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:26:13,679 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:27:01,968 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:27:59,626 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:29:11,314 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:29:50,439 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:30:33,713 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:31:42,482 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:35:06,224 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:35:10,416 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:35:14,938 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:35:31,934 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:35:42,777 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:35:54,742 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:36:06,050 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:36:18,637 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:36:48,575 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:37:35,293 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:39:31,887 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:42:28,184 - ERROR - Exception raised in Job[18]: TimeoutError()\n",
      "2025-12-02 01:42:35,977 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:42:39,989 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:42:43,554 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:43:00,822 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:43:14,602 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:43:25,830 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:43:37,944 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:43:50,063 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:44:20,443 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:44:39,342 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:45:44,311 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:46:25,065 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:46:29,381 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-12-02 01:46:33,748 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation complete.\n",
      "\n",
      "=== Per-sample metrics ===\n",
      "                                                        user_input  context_precision  context_recall  faithfulness  answer_relevancy\n",
      "0                What is the main purpose of the requests library?           0.916667             1.0      0.714286          0.989356\n",
      "1  How does the Session object persist parameters across requests?           0.804167             1.0           NaN          0.937942\n",
      "2         What is the role of the `requests.adapters.HTTPAdapter`?           0.679167             0.5      1.000000          0.994467\n",
      "3                     How can you specify a timeout for a request?           0.887500             1.0      0.833333          0.921742\n",
      "4                 How are cookies handled in the requests library?           1.000000             0.5           NaN          0.950434\n",
      "5              What file defines the main `requests.get` function?           0.950000             0.0      0.333333          0.938500\n",
      "\n",
      "=== Averages ===\n",
      "context_precision    0.872917\n",
      "context_recall       0.666667\n",
      "faithfulness         0.720238\n",
      "answer_relevancy     0.955407\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the main purpose of the requests library?</td>\n",
       "      <td>[# Requests\\r\\n\\r\\n**Requests** is a simple, yet elegant, HTTP library.\\r\\n\\r\\n```python\\r\\n&gt;&gt;&gt; import requests\\r\\n&gt;&gt;&gt; r = requests.get('https://httpbin.org/basic-auth/user/pass', auth=('user', 'pass'))\\r\\n&gt;&gt;&gt; r.status_code\\r\\n200\\r\\n&gt;&gt;&gt; r.headers['content-type']\\r\\n'application/json; charset=utf8'\\r\\n&gt;&gt;&gt; r.encoding\\r\\n'utf-8'\\r\\n&gt;&gt;&gt; r.text\\r\\n'{\"authenticated\": true, ...'\\r\\n&gt;&gt;&gt; r.json()\\r\\n{...</td>\n",
       "      <td>The **Requests** library is a Python HTTP client whose main purpose is to provide a simple, human‚Äëfriendly way to send HTTP/1.1 requests (GET, POST, etc.) and handle the responses.\\n\\n- The README describes it as ‚Äúa simple, yet elegant, HTTP library.‚Äù  \\n- The module docstring in `src\\requests\\__init__.py` says: ‚ÄúRequests is an HTTP library, written in Python, for human beings.‚Äù  \\n\\nThus, its...</td>\n",
       "      <td>Requests is an HTTP library for Python, built for human beings. It allows you to send HTTP/1.1 requests extremely easily.</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.989356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does the Session object persist parameters across requests?</td>\n",
       "      <td>[\"\"\"\\r\\nrequests.sessions\\r\\n~~~~~~~~~~~~~~~~~\\r\\n\\r\\nThis module provides a Session object to manage and persist settings across\\r\\nrequests (cookies, auth, proxies).\\r\\n\"\"\"\\r\\nimport os\\r\\nimport sys\\r\\nimport time\\r\\nfrom collections import OrderedDict\\r\\nfrom datetime import timedelta\\r\\n\\r\\nfrom ._internal_utils import to_native_string\\r\\nfrom .adapters import HTTPAdapter\\r\\nfrom .auth im...</td>\n",
       "      <td>**How the `Session` object keeps parameters alive between calls**\\n\\nIn `src\\requests\\sessions.py` the `Session` class stores a set of attributes that act as the *default* values for every request made through that session:\\n\\n```python\\ndef __init__(self):\\n    self.headers = default_headers()          # default headers\\n    self.auth = None                           # default auth tuple / ob...</td>\n",
       "      <td>A Session object has a variety of methods for customizing requests, such as setting headers, auth, cookies, and proxies. These settings are persisted across all requests made with that session instance.</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.937942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the role of the `requests.adapters.HTTPAdapter`?</td>\n",
       "      <td>[def request_url(self, request, proxies):\\r\\n        \"\"\"Obtain the url to use when making the final request.\\r\\n\\r\\n        If the message is being sent through a HTTP proxy, the full URL has to\\r\\n        be used. Otherwise, we should only use the path portion of the URL.\\r\\n\\r\\n        This should not be called from user code, and is only exposed for use\\r\\n        when subclassing the\\r\\n  ...</td>\n",
       "      <td>**Role of `requests.adapters.HTTPAdapter`**\\n\\n`HTTPAdapter` is the built‚Äëin transport adapter that lets a **Requests** `Session` actually talk to HTTP/HTTPS endpoints.  \\n\\n- It implements the *Transport Adapter* interface required by a `Session`.  \\n- It creates and manages urllib3 connection pools (via `init_poolmanager`) and, when needed, proxy managers.  \\n- It handles proxy selection, UR...</td>\n",
       "      <td>The HTTPAdapter is responsible for the actual transport of the request. It sends the request to the target server and handles connection pooling.</td>\n",
       "      <td>0.679167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can you specify a timeout for a request?</td>\n",
       "      <td>[class TestTimeout:\\r\\n    def test_stream_timeout(self, httpbin):\\r\\n        try:\\r\\n            requests.get(httpbin(\"delay/10\"), timeout=2.0)\\r\\n        except requests.exceptions.Timeout as e:\\r\\n            assert \"Read timed out\" in e.args[0].args[0]\\r\\n\\r\\n    @pytest.mark.parametrize(\\r\\n        \"timeout, error_text\",\\r\\n        (\\r\\n            ((3, 4, 5), \"(connect, read)\"),\\r\\n     ...</td>\n",
       "      <td>You set the timeout by passing a **`timeout`** argument to the request‚Äëmaking function (e.g. `requests.get`, `requests.post`, `requests.request`, or the `Session.request` method).\\n\\n**What you can pass**\\n\\n| Type | Meaning |\\n|------|---------|\\n| `float` (or `int`) | The same number of seconds is used for both the *connect* and *read* phases. |\\n| `tuple` `(connect, read)` | Separate values...</td>\n",
       "      <td>You can tell Requests to stop waiting for a response after a given number of seconds with the `timeout` parameter. It can be a float for a connect and read timeout, or a tuple `(connect_timeout, read_timeout)`.</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.921742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How are cookies handled in the requests library?</td>\n",
       "      <td>[\"\"\"\\r\\nrequests.cookies\\r\\n~~~~~~~~~~~~~~~~\\r\\n\\r\\nCompatibility code to be able to use `http.cookiejar.CookieJar` with requests.\\r\\n\\r\\nrequests.utils imports from here, so be careful with imports.\\r\\n\"\"\"\\r\\n\\r\\nimport calendar\\r\\nimport copy\\r\\nimport time\\r\\n\\r\\nfrom ._internal_utils import to_native_string\\r\\nfrom .compat import Morsel, MutableMapping, cookielib, urlparse, urlunparse\\r\\n\\...</td>\n",
       "      <td>**Cookie handling in `requests` is built on top of Python‚Äôs `http.cookiejar.CookieJar` but wrapped with a few helper classes so that the rest of the library can work with a simple dict‚Äëlike API.**  \\n\\nBelow is a concise walk‚Äëthrough of the relevant pieces that appear in the source files you provided.\\n\\n---\\n\\n### 1. The underlying jar ‚Äì `RequestsCookieJar`\\n\\n*File: `src\\requests\\cookies.py`...</td>\n",
       "      <td>Cookies are returned in a `RequestsCookieJar`, which acts like a dictionary but also works across domains and paths. Session objects also persist cookies across all requests.</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What file defines the main `requests.get` function?</td>\n",
       "      <td>[def request(method, url, **kwargs):\\r\\n    \"\"\"Constructs and sends a :class:`Request &lt;Request&gt;`.\\r\\n\\r\\n    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.\\r\\n    :param url: URL for the new :class:`Request` object.\\r\\n    :param params: (optional) Dictionary, list of tuples or bytes to send\\r\\n        in ...</td>\n",
       "      <td>The top‚Äëlevel **`requests.get`** function is defined in **`src\\requests\\api.py`**:\\n\\n```python\\ndef get(url, params=None, **kwargs):\\n    \"\"\"Sends a GET request.\"\"\"\\n    return request(\"get\", url, params=params, **kwargs)\\n```\\n\\n(See the `get` definition in `src\\requests\\api.py`.)</td>\n",
       "      <td>The `get` function is a wrapper defined in `requests/api.py` that calls the `request` function with the method set to 'GET'.</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.938500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        user_input                                                                                                                                                                                                                                                                                                                                                                                               retrieved_contexts                                                                                                                                                                                                                                                                                                                                                                                                         response                                                                                                                                                                                                           reference  context_precision  context_recall  faithfulness  answer_relevancy\n",
       "0                What is the main purpose of the requests library?  [# Requests\\r\\n\\r\\n**Requests** is a simple, yet elegant, HTTP library.\\r\\n\\r\\n```python\\r\\n>>> import requests\\r\\n>>> r = requests.get('https://httpbin.org/basic-auth/user/pass', auth=('user', 'pass'))\\r\\n>>> r.status_code\\r\\n200\\r\\n>>> r.headers['content-type']\\r\\n'application/json; charset=utf8'\\r\\n>>> r.encoding\\r\\n'utf-8'\\r\\n>>> r.text\\r\\n'{\"authenticated\": true, ...'\\r\\n>>> r.json()\\r\\n{...  The **Requests** library is a Python HTTP client whose main purpose is to provide a simple, human‚Äëfriendly way to send HTTP/1.1 requests (GET, POST, etc.) and handle the responses.\\n\\n- The README describes it as ‚Äúa simple, yet elegant, HTTP library.‚Äù  \\n- The module docstring in `src\\requests\\__init__.py` says: ‚ÄúRequests is an HTTP library, written in Python, for human beings.‚Äù  \\n\\nThus, its...                                                                                           Requests is an HTTP library for Python, built for human beings. It allows you to send HTTP/1.1 requests extremely easily.           0.916667             1.0      0.714286          0.989356\n",
       "1  How does the Session object persist parameters across requests?  [\"\"\"\\r\\nrequests.sessions\\r\\n~~~~~~~~~~~~~~~~~\\r\\n\\r\\nThis module provides a Session object to manage and persist settings across\\r\\nrequests (cookies, auth, proxies).\\r\\n\"\"\"\\r\\nimport os\\r\\nimport sys\\r\\nimport time\\r\\nfrom collections import OrderedDict\\r\\nfrom datetime import timedelta\\r\\n\\r\\nfrom ._internal_utils import to_native_string\\r\\nfrom .adapters import HTTPAdapter\\r\\nfrom .auth im...  **How the `Session` object keeps parameters alive between calls**\\n\\nIn `src\\requests\\sessions.py` the `Session` class stores a set of attributes that act as the *default* values for every request made through that session:\\n\\n```python\\ndef __init__(self):\\n    self.headers = default_headers()          # default headers\\n    self.auth = None                           # default auth tuple / ob...          A Session object has a variety of methods for customizing requests, such as setting headers, auth, cookies, and proxies. These settings are persisted across all requests made with that session instance.           0.804167             1.0           NaN          0.937942\n",
       "2         What is the role of the `requests.adapters.HTTPAdapter`?  [def request_url(self, request, proxies):\\r\\n        \"\"\"Obtain the url to use when making the final request.\\r\\n\\r\\n        If the message is being sent through a HTTP proxy, the full URL has to\\r\\n        be used. Otherwise, we should only use the path portion of the URL.\\r\\n\\r\\n        This should not be called from user code, and is only exposed for use\\r\\n        when subclassing the\\r\\n  ...  **Role of `requests.adapters.HTTPAdapter`**\\n\\n`HTTPAdapter` is the built‚Äëin transport adapter that lets a **Requests** `Session` actually talk to HTTP/HTTPS endpoints.  \\n\\n- It implements the *Transport Adapter* interface required by a `Session`.  \\n- It creates and manages urllib3 connection pools (via `init_poolmanager`) and, when needed, proxy managers.  \\n- It handles proxy selection, UR...                                                                   The HTTPAdapter is responsible for the actual transport of the request. It sends the request to the target server and handles connection pooling.           0.679167             0.5      1.000000          0.994467\n",
       "3                     How can you specify a timeout for a request?  [class TestTimeout:\\r\\n    def test_stream_timeout(self, httpbin):\\r\\n        try:\\r\\n            requests.get(httpbin(\"delay/10\"), timeout=2.0)\\r\\n        except requests.exceptions.Timeout as e:\\r\\n            assert \"Read timed out\" in e.args[0].args[0]\\r\\n\\r\\n    @pytest.mark.parametrize(\\r\\n        \"timeout, error_text\",\\r\\n        (\\r\\n            ((3, 4, 5), \"(connect, read)\"),\\r\\n     ...  You set the timeout by passing a **`timeout`** argument to the request‚Äëmaking function (e.g. `requests.get`, `requests.post`, `requests.request`, or the `Session.request` method).\\n\\n**What you can pass**\\n\\n| Type | Meaning |\\n|------|---------|\\n| `float` (or `int`) | The same number of seconds is used for both the *connect* and *read* phases. |\\n| `tuple` `(connect, read)` | Separate values...  You can tell Requests to stop waiting for a response after a given number of seconds with the `timeout` parameter. It can be a float for a connect and read timeout, or a tuple `(connect_timeout, read_timeout)`.           0.887500             1.0      0.833333          0.921742\n",
       "4                 How are cookies handled in the requests library?  [\"\"\"\\r\\nrequests.cookies\\r\\n~~~~~~~~~~~~~~~~\\r\\n\\r\\nCompatibility code to be able to use `http.cookiejar.CookieJar` with requests.\\r\\n\\r\\nrequests.utils imports from here, so be careful with imports.\\r\\n\"\"\"\\r\\n\\r\\nimport calendar\\r\\nimport copy\\r\\nimport time\\r\\n\\r\\nfrom ._internal_utils import to_native_string\\r\\nfrom .compat import Morsel, MutableMapping, cookielib, urlparse, urlunparse\\r\\n\\...  **Cookie handling in `requests` is built on top of Python‚Äôs `http.cookiejar.CookieJar` but wrapped with a few helper classes so that the rest of the library can work with a simple dict‚Äëlike API.**  \\n\\nBelow is a concise walk‚Äëthrough of the relevant pieces that appear in the source files you provided.\\n\\n---\\n\\n### 1. The underlying jar ‚Äì `RequestsCookieJar`\\n\\n*File: `src\\requests\\cookies.py`...                                      Cookies are returned in a `RequestsCookieJar`, which acts like a dictionary but also works across domains and paths. Session objects also persist cookies across all requests.           1.000000             0.5           NaN          0.950434\n",
       "5              What file defines the main `requests.get` function?  [def request(method, url, **kwargs):\\r\\n    \"\"\"Constructs and sends a :class:`Request <Request>`.\\r\\n\\r\\n    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.\\r\\n    :param url: URL for the new :class:`Request` object.\\r\\n    :param params: (optional) Dictionary, list of tuples or bytes to send\\r\\n        in ...                                                                                                                      The top‚Äëlevel **`requests.get`** function is defined in **`src\\requests\\api.py`**:\\n\\n```python\\ndef get(url, params=None, **kwargs):\\n    \"\"\"Sends a GET request.\"\"\"\\n    return request(\"get\", url, params=params, **kwargs)\\n```\\n\\n(See the `get` definition in `src\\requests\\api.py`.)                                                                                        The `get` function is a wrapper defined in `requests/api.py` that calls the `request` function with the method set to 'GET'.           0.950000             0.0      0.333333          0.938500"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========== FULL RAGAS EVALUATION (ALL SAMPLES) ==========\n",
    "import os\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import HuggingFaceEmbeddings\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. PREPARE THE DATASET\n",
    "# -----------------------------------------------------------\n",
    "ds = eval_dataset  # already created earlier\n",
    "\n",
    "# Expected ‚Üí actual column rename mapping\n",
    "rename_map = {\n",
    "    \"question\": \"user_input\",\n",
    "    \"answer\": \"response\",\n",
    "    \"contexts\": \"retrieved_contexts\",\n",
    "    \"ground_truth\": \"reference\",\n",
    "}\n",
    "\n",
    "for old, new in rename_map.items():\n",
    "    if old in ds.column_names and new not in ds.column_names:\n",
    "        ds = ds.rename_column(old, new)\n",
    "\n",
    "# Ensure retrieved_contexts is list[str]\n",
    "def ensure_list(example):\n",
    "    rc = example.get(\"retrieved_contexts\")\n",
    "    if rc is None:\n",
    "        example[\"retrieved_contexts\"] = []\n",
    "    elif isinstance(rc, str):\n",
    "        example[\"retrieved_contexts\"] = [rc]\n",
    "    return example\n",
    "\n",
    "if \"retrieved_contexts\" in ds.column_names:\n",
    "    ds = ds.map(ensure_list)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. JUDGE (LLAMA VIA OLLAMA)\n",
    "# -----------------------------------------------------------\n",
    "judge_chat = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "judge_llm = LangchainLLMWrapper(judge_chat)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. EMBEDDINGS (FOR SIMILARITY METRICS)\n",
    "# -----------------------------------------------------------\n",
    "emb = HuggingFaceEmbeddings(model=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. RUN CONFIG\n",
    "# -----------------------------------------------------------\n",
    "run_cfg = RunConfig(\n",
    "    max_workers=1,       # sequential (stable)\n",
    "    timeout=300,         # 2.5 min per call to avoid timeouts\n",
    "    max_wait=200,\n",
    "    max_retries=1,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5. RUN FULL EVALUATION\n",
    "# -----------------------------------------------------------\n",
    "print(\"\\nüöÄ Running FULL RAGAS evaluation on all samples...\")\n",
    "result = evaluate(\n",
    "    dataset=ds,\n",
    "    metrics=[context_precision, context_recall, faithfulness, answer_relevancy],\n",
    "    llm=judge_llm,\n",
    "    embeddings=emb,\n",
    "    run_config=run_cfg,\n",
    "    raise_exceptions=False,   # Full run ‚Üí don't stop mid-way\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete.\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6. SHOW RESULTS\n",
    "# -----------------------------------------------------------\n",
    "df = result.to_pandas()\n",
    "print(\"\\n=== Per-sample metrics ===\")\n",
    "print(df[['user_input', 'context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']])\n",
    "\n",
    "print(\"\\n=== Averages ===\")\n",
    "print(df[['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']].mean())\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metrics written to ragas_metrics_summary.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# df = evaluation_df   # ensure this variable exists\n",
    "\n",
    "summary_file = \"ragas_metrics_summary.txt\"\n",
    "\n",
    "# Select only the four RAGAS metric columns (ignore everything else)\n",
    "metric_cols = [\"context_precision\", \"context_recall\", \"faithfulness\", \"answer_relevancy\"]\n",
    "\n",
    "# Compute averages safely (skip missing columns)\n",
    "metrics_present = [m for m in metric_cols if m in df.columns]\n",
    "metric_means = df[metrics_present].mean(skipna=True)\n",
    "\n",
    "# Write to txt\n",
    "with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== RAGAS Evaluation Metrics Summary ===\\n\\n\")\n",
    "    for metric, value in metric_means.items():\n",
    "        f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Metrics written to {summary_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
